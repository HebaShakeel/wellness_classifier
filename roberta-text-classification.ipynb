{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9629263,"sourceType":"datasetVersion","datasetId":5878255}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:02:34.973052Z","iopub.execute_input":"2024-11-04T05:02:34.973361Z","iopub.status.idle":"2024-11-04T05:02:48.848206Z","shell.execute_reply.started":"2024-11-04T05:02:34.973328Z","shell.execute_reply":"2024-11-04T05:02:48.847091Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n\nimport evaluate\nimport numpy as np\nfrom transformers import DataCollatorWithPadding","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:14:48.017805Z","iopub.execute_input":"2024-11-04T05:14:48.018200Z","iopub.status.idle":"2024-11-04T05:14:48.023895Z","shell.execute_reply.started":"2024-11-04T05:14:48.018162Z","shell.execute_reply":"2024-11-04T05:14:48.022948Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_dict = load_dataset(\"hebashakeel/Bert-classification-wellness\")","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:14:50.932185Z","iopub.execute_input":"2024-11-04T05:14:50.932837Z","iopub.status.idle":"2024-11-04T05:15:04.429517Z","shell.execute_reply.started":"2024-11-04T05:14:50.932794Z","shell.execute_reply":"2024-11-04T05:15:04.428562Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dataset_dict","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:15:04.431165Z","iopub.execute_input":"2024-11-04T05:15:04.431479Z","iopub.status.idle":"2024-11-04T05:15:04.438426Z","shell.execute_reply.started":"2024-11-04T05:15:04.431446Z","shell.execute_reply":"2024-11-04T05:15:04.437473Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 1118\n    })\n    validation: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 239\n    })\n    test: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 241\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification\nfrom transformers import RobertaTokenizer\n\n# Initializing a RoBERTa configuration\nconfiguration = RobertaConfig()\n\n# Initializing a model (with random weights) from the configuration\n# model = RobertaModel(configuration)\n\n# Define dropout rates\nconfig = RobertaConfig.from_pretrained(\n    'roberta-base',\n    hidden_dropout_prob=0.5,         # Dropout in fully connected layers\n    attention_probs_dropout_prob=0.5, # Dropout in attention probabilities\n    num_labels=4,\n    id2label=id2label, \n    label2id=label2id,\n)\n\nid2label = {0: \"PA\", 1: \"IVA\", 2: \"SA\", 3: \"SEA\"}\nlabel2id = {\"PA\": 0, \"IVA\": 1, \"SA\": 2, \"SEA\": 3}\n\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", config = config)\n\n# Accessing the model configuration\nconfiguration = model.config\n\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:15:04.439414Z","iopub.execute_input":"2024-11-04T05:15:04.439756Z","iopub.status.idle":"2024-11-04T05:15:08.601039Z","shell.execute_reply.started":"2024-11-04T05:15:04.439716Z","shell.execute_reply":"2024-11-04T05:15:08.600014Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# print layers\nfor name, param in model.named_parameters():\n   print(name, param.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:15:08.602897Z","iopub.execute_input":"2024-11-04T05:15:08.603212Z","iopub.status.idle":"2024-11-04T05:15:08.673135Z","shell.execute_reply.started":"2024-11-04T05:15:08.603174Z","shell.execute_reply":"2024-11-04T05:15:08.672294Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"roberta.embeddings.word_embeddings.weight True\nroberta.embeddings.position_embeddings.weight True\nroberta.embeddings.token_type_embeddings.weight True\nroberta.embeddings.LayerNorm.weight True\nroberta.embeddings.LayerNorm.bias True\nroberta.encoder.layer.0.attention.self.query.weight True\nroberta.encoder.layer.0.attention.self.query.bias True\nroberta.encoder.layer.0.attention.self.key.weight True\nroberta.encoder.layer.0.attention.self.key.bias True\nroberta.encoder.layer.0.attention.self.value.weight True\nroberta.encoder.layer.0.attention.self.value.bias True\nroberta.encoder.layer.0.attention.output.dense.weight True\nroberta.encoder.layer.0.attention.output.dense.bias True\nroberta.encoder.layer.0.attention.output.LayerNorm.weight True\nroberta.encoder.layer.0.attention.output.LayerNorm.bias True\nroberta.encoder.layer.0.intermediate.dense.weight True\nroberta.encoder.layer.0.intermediate.dense.bias True\nroberta.encoder.layer.0.output.dense.weight True\nroberta.encoder.layer.0.output.dense.bias True\nroberta.encoder.layer.0.output.LayerNorm.weight True\nroberta.encoder.layer.0.output.LayerNorm.bias True\nroberta.encoder.layer.1.attention.self.query.weight True\nroberta.encoder.layer.1.attention.self.query.bias True\nroberta.encoder.layer.1.attention.self.key.weight True\nroberta.encoder.layer.1.attention.self.key.bias True\nroberta.encoder.layer.1.attention.self.value.weight True\nroberta.encoder.layer.1.attention.self.value.bias True\nroberta.encoder.layer.1.attention.output.dense.weight True\nroberta.encoder.layer.1.attention.output.dense.bias True\nroberta.encoder.layer.1.attention.output.LayerNorm.weight True\nroberta.encoder.layer.1.attention.output.LayerNorm.bias True\nroberta.encoder.layer.1.intermediate.dense.weight True\nroberta.encoder.layer.1.intermediate.dense.bias True\nroberta.encoder.layer.1.output.dense.weight True\nroberta.encoder.layer.1.output.dense.bias True\nroberta.encoder.layer.1.output.LayerNorm.weight True\nroberta.encoder.layer.1.output.LayerNorm.bias True\nroberta.encoder.layer.2.attention.self.query.weight True\nroberta.encoder.layer.2.attention.self.query.bias True\nroberta.encoder.layer.2.attention.self.key.weight True\nroberta.encoder.layer.2.attention.self.key.bias True\nroberta.encoder.layer.2.attention.self.value.weight True\nroberta.encoder.layer.2.attention.self.value.bias True\nroberta.encoder.layer.2.attention.output.dense.weight True\nroberta.encoder.layer.2.attention.output.dense.bias True\nroberta.encoder.layer.2.attention.output.LayerNorm.weight True\nroberta.encoder.layer.2.attention.output.LayerNorm.bias True\nroberta.encoder.layer.2.intermediate.dense.weight True\nroberta.encoder.layer.2.intermediate.dense.bias True\nroberta.encoder.layer.2.output.dense.weight True\nroberta.encoder.layer.2.output.dense.bias True\nroberta.encoder.layer.2.output.LayerNorm.weight True\nroberta.encoder.layer.2.output.LayerNorm.bias True\nroberta.encoder.layer.3.attention.self.query.weight True\nroberta.encoder.layer.3.attention.self.query.bias True\nroberta.encoder.layer.3.attention.self.key.weight True\nroberta.encoder.layer.3.attention.self.key.bias True\nroberta.encoder.layer.3.attention.self.value.weight True\nroberta.encoder.layer.3.attention.self.value.bias True\nroberta.encoder.layer.3.attention.output.dense.weight True\nroberta.encoder.layer.3.attention.output.dense.bias True\nroberta.encoder.layer.3.attention.output.LayerNorm.weight True\nroberta.encoder.layer.3.attention.output.LayerNorm.bias True\nroberta.encoder.layer.3.intermediate.dense.weight True\nroberta.encoder.layer.3.intermediate.dense.bias True\nroberta.encoder.layer.3.output.dense.weight True\nroberta.encoder.layer.3.output.dense.bias True\nroberta.encoder.layer.3.output.LayerNorm.weight True\nroberta.encoder.layer.3.output.LayerNorm.bias True\nroberta.encoder.layer.4.attention.self.query.weight True\nroberta.encoder.layer.4.attention.self.query.bias True\nroberta.encoder.layer.4.attention.self.key.weight True\nroberta.encoder.layer.4.attention.self.key.bias True\nroberta.encoder.layer.4.attention.self.value.weight True\nroberta.encoder.layer.4.attention.self.value.bias True\nroberta.encoder.layer.4.attention.output.dense.weight True\nroberta.encoder.layer.4.attention.output.dense.bias True\nroberta.encoder.layer.4.attention.output.LayerNorm.weight True\nroberta.encoder.layer.4.attention.output.LayerNorm.bias True\nroberta.encoder.layer.4.intermediate.dense.weight True\nroberta.encoder.layer.4.intermediate.dense.bias True\nroberta.encoder.layer.4.output.dense.weight True\nroberta.encoder.layer.4.output.dense.bias True\nroberta.encoder.layer.4.output.LayerNorm.weight True\nroberta.encoder.layer.4.output.LayerNorm.bias True\nroberta.encoder.layer.5.attention.self.query.weight True\nroberta.encoder.layer.5.attention.self.query.bias True\nroberta.encoder.layer.5.attention.self.key.weight True\nroberta.encoder.layer.5.attention.self.key.bias True\nroberta.encoder.layer.5.attention.self.value.weight True\nroberta.encoder.layer.5.attention.self.value.bias True\nroberta.encoder.layer.5.attention.output.dense.weight True\nroberta.encoder.layer.5.attention.output.dense.bias True\nroberta.encoder.layer.5.attention.output.LayerNorm.weight True\nroberta.encoder.layer.5.attention.output.LayerNorm.bias True\nroberta.encoder.layer.5.intermediate.dense.weight True\nroberta.encoder.layer.5.intermediate.dense.bias True\nroberta.encoder.layer.5.output.dense.weight True\nroberta.encoder.layer.5.output.dense.bias True\nroberta.encoder.layer.5.output.LayerNorm.weight True\nroberta.encoder.layer.5.output.LayerNorm.bias True\nroberta.encoder.layer.6.attention.self.query.weight True\nroberta.encoder.layer.6.attention.self.query.bias True\nroberta.encoder.layer.6.attention.self.key.weight True\nroberta.encoder.layer.6.attention.self.key.bias True\nroberta.encoder.layer.6.attention.self.value.weight True\nroberta.encoder.layer.6.attention.self.value.bias True\nroberta.encoder.layer.6.attention.output.dense.weight True\nroberta.encoder.layer.6.attention.output.dense.bias True\nroberta.encoder.layer.6.attention.output.LayerNorm.weight True\nroberta.encoder.layer.6.attention.output.LayerNorm.bias True\nroberta.encoder.layer.6.intermediate.dense.weight True\nroberta.encoder.layer.6.intermediate.dense.bias True\nroberta.encoder.layer.6.output.dense.weight True\nroberta.encoder.layer.6.output.dense.bias True\nroberta.encoder.layer.6.output.LayerNorm.weight True\nroberta.encoder.layer.6.output.LayerNorm.bias True\nroberta.encoder.layer.7.attention.self.query.weight True\nroberta.encoder.layer.7.attention.self.query.bias True\nroberta.encoder.layer.7.attention.self.key.weight True\nroberta.encoder.layer.7.attention.self.key.bias True\nroberta.encoder.layer.7.attention.self.value.weight True\nroberta.encoder.layer.7.attention.self.value.bias True\nroberta.encoder.layer.7.attention.output.dense.weight True\nroberta.encoder.layer.7.attention.output.dense.bias True\nroberta.encoder.layer.7.attention.output.LayerNorm.weight True\nroberta.encoder.layer.7.attention.output.LayerNorm.bias True\nroberta.encoder.layer.7.intermediate.dense.weight True\nroberta.encoder.layer.7.intermediate.dense.bias True\nroberta.encoder.layer.7.output.dense.weight True\nroberta.encoder.layer.7.output.dense.bias True\nroberta.encoder.layer.7.output.LayerNorm.weight True\nroberta.encoder.layer.7.output.LayerNorm.bias True\nroberta.encoder.layer.8.attention.self.query.weight True\nroberta.encoder.layer.8.attention.self.query.bias True\nroberta.encoder.layer.8.attention.self.key.weight True\nroberta.encoder.layer.8.attention.self.key.bias True\nroberta.encoder.layer.8.attention.self.value.weight True\nroberta.encoder.layer.8.attention.self.value.bias True\nroberta.encoder.layer.8.attention.output.dense.weight True\nroberta.encoder.layer.8.attention.output.dense.bias True\nroberta.encoder.layer.8.attention.output.LayerNorm.weight True\nroberta.encoder.layer.8.attention.output.LayerNorm.bias True\nroberta.encoder.layer.8.intermediate.dense.weight True\nroberta.encoder.layer.8.intermediate.dense.bias True\nroberta.encoder.layer.8.output.dense.weight True\nroberta.encoder.layer.8.output.dense.bias True\nroberta.encoder.layer.8.output.LayerNorm.weight True\nroberta.encoder.layer.8.output.LayerNorm.bias True\nroberta.encoder.layer.9.attention.self.query.weight True\nroberta.encoder.layer.9.attention.self.query.bias True\nroberta.encoder.layer.9.attention.self.key.weight True\nroberta.encoder.layer.9.attention.self.key.bias True\nroberta.encoder.layer.9.attention.self.value.weight True\nroberta.encoder.layer.9.attention.self.value.bias True\nroberta.encoder.layer.9.attention.output.dense.weight True\nroberta.encoder.layer.9.attention.output.dense.bias True\nroberta.encoder.layer.9.attention.output.LayerNorm.weight True\nroberta.encoder.layer.9.attention.output.LayerNorm.bias True\nroberta.encoder.layer.9.intermediate.dense.weight True\nroberta.encoder.layer.9.intermediate.dense.bias True\nroberta.encoder.layer.9.output.dense.weight True\nroberta.encoder.layer.9.output.dense.bias True\nroberta.encoder.layer.9.output.LayerNorm.weight True\nroberta.encoder.layer.9.output.LayerNorm.bias True\nroberta.encoder.layer.10.attention.self.query.weight True\nroberta.encoder.layer.10.attention.self.query.bias True\nroberta.encoder.layer.10.attention.self.key.weight True\nroberta.encoder.layer.10.attention.self.key.bias True\nroberta.encoder.layer.10.attention.self.value.weight True\nroberta.encoder.layer.10.attention.self.value.bias True\nroberta.encoder.layer.10.attention.output.dense.weight True\nroberta.encoder.layer.10.attention.output.dense.bias True\nroberta.encoder.layer.10.attention.output.LayerNorm.weight True\nroberta.encoder.layer.10.attention.output.LayerNorm.bias True\nroberta.encoder.layer.10.intermediate.dense.weight True\nroberta.encoder.layer.10.intermediate.dense.bias True\nroberta.encoder.layer.10.output.dense.weight True\nroberta.encoder.layer.10.output.dense.bias True\nroberta.encoder.layer.10.output.LayerNorm.weight True\nroberta.encoder.layer.10.output.LayerNorm.bias True\nroberta.encoder.layer.11.attention.self.query.weight True\nroberta.encoder.layer.11.attention.self.query.bias True\nroberta.encoder.layer.11.attention.self.key.weight True\nroberta.encoder.layer.11.attention.self.key.bias True\nroberta.encoder.layer.11.attention.self.value.weight True\nroberta.encoder.layer.11.attention.self.value.bias True\nroberta.encoder.layer.11.attention.output.dense.weight True\nroberta.encoder.layer.11.attention.output.dense.bias True\nroberta.encoder.layer.11.attention.output.LayerNorm.weight True\nroberta.encoder.layer.11.attention.output.LayerNorm.bias True\nroberta.encoder.layer.11.intermediate.dense.weight True\nroberta.encoder.layer.11.intermediate.dense.bias True\nroberta.encoder.layer.11.output.dense.weight True\nroberta.encoder.layer.11.output.dense.bias True\nroberta.encoder.layer.11.output.LayerNorm.weight True\nroberta.encoder.layer.11.output.LayerNorm.bias True\nclassifier.dense.weight True\nclassifier.dense.bias True\nclassifier.out_proj.weight True\nclassifier.out_proj.bias True\n","output_type":"stream"}]},{"cell_type":"code","source":"# define text preprocessing\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:15:26.772470Z","iopub.execute_input":"2024-11-04T05:15:26.772869Z","iopub.status.idle":"2024-11-04T05:15:26.778341Z","shell.execute_reply.started":"2024-11-04T05:15:26.772830Z","shell.execute_reply":"2024-11-04T05:15:26.777484Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# tokenize all datasets\ntokenized_data = dataset_dict.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:15:30.409708Z","iopub.execute_input":"2024-11-04T05:15:30.410436Z","iopub.status.idle":"2024-11-04T05:15:37.332920Z","shell.execute_reply.started":"2024-11-04T05:15:30.410393Z","shell.execute_reply":"2024-11-04T05:15:37.332083Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# create data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:15:37.852848Z","iopub.execute_input":"2024-11-04T05:15:37.853300Z","iopub.status.idle":"2024-11-04T05:15:37.859504Z","shell.execute_reply.started":"2024-11-04T05:15:37.853248Z","shell.execute_reply":"2024-11-04T05:15:37.858470Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from scipy.special import softmax\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n\n    # Apply softmax to get probabilities\n    probabilities = softmax(predictions, axis=1)\n    \n    # Compute AUC for each class in a one-vs-rest fashion\n    aucs = []\n    for class_idx in range(4): \n        binary_labels = (labels == class_idx).astype(int)\n        auc = auc_score.compute(prediction_scores=probabilities[:, class_idx], references=binary_labels)['roc_auc']\n        aucs.append(auc)\n    avg_auc = np.round(np.mean(aucs), 3)\n    \n    # Predict most probable class\n    predicted_classes = np.argmax(predictions, axis=1)\n    \n    # Compute accuracy\n    acc = np.round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'], 3)\n    \n    # Compute class-wise precision and recall\n    class_precision = {}\n    class_recall = {}\n    for class_idx in range(4):\n        tp = np.sum((predicted_classes == class_idx) & (labels == class_idx))\n        fp = np.sum((predicted_classes == class_idx) & (labels != class_idx))\n        fn = np.sum((predicted_classes != class_idx) & (labels == class_idx))\n        \n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n        \n        class_precision[f\"Precision_Class_{class_idx}\"] = np.round(precision, 3)\n        class_recall[f\"Recall_Class_{class_idx}\"] = np.round(recall, 3)\n    \n    return {\n        \"Accuracy\": acc, \n        \"AUC\": avg_auc,\n        **class_precision,\n        **class_recall\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:15:41.748022Z","iopub.execute_input":"2024-11-04T05:15:41.748416Z","iopub.status.idle":"2024-11-04T05:15:41.760389Z","shell.execute_reply.started":"2024-11-04T05:15:41.748380Z","shell.execute_reply":"2024-11-04T05:15:41.759358Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# hyperparameters\nlr = 3e-5 \nbatch_size = 8 \nnum_epochs = 10 \n\ntraining_args = TrainingArguments(\n    output_dir=\"roberta-wellness-classifier\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    logging_strategy=\"epoch\",  # Change to \"steps\" if troubleshooting\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:15:46.895683Z","iopub.execute_input":"2024-11-04T05:15:46.896650Z","iopub.status.idle":"2024-11-04T05:15:46.928615Z","shell.execute_reply.started":"2024-11-04T05:15:46.896596Z","shell.execute_reply":"2024-11-04T05:15:46.927832Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_data[\"train\"],\n    eval_dataset=tokenized_data[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n) \n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T05:15:49.976575Z","iopub.execute_input":"2024-11-04T05:15:49.977456Z","iopub.status.idle":"2024-11-04T05:16:02.219106Z","shell.execute_reply.started":"2024-11-04T05:15:49.977417Z","shell.execute_reply":"2024-11-04T05:16:02.217612Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='141' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 141/1400 00:10 < 01:35, 13.14 it/s, Epoch 1/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [31/31 00:00]\n    </div>\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      9\u001b[0m ) \n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2487\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2491\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2915\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2913\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2915\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2918\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2872\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2872\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2873\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2875\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3869\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:4160\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4156\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   4157\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   4158\u001b[0m         )\n\u001b[1;32m   4159\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4160\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4162\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n","Cell \u001b[0;32mIn[28], line 13\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m): \n\u001b[1;32m     12\u001b[0m     binary_labels \u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m==\u001b[39m class_idx)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     auc \u001b[38;5;241m=\u001b[39m \u001b[43mauc_score\u001b[49m\u001b[38;5;241m.\u001b[39mcompute(prediction_scores\u001b[38;5;241m=\u001b[39mprobabilities[:, class_idx], references\u001b[38;5;241m=\u001b[39mbinary_labels)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m     aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[1;32m     15\u001b[0m avg_auc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(np\u001b[38;5;241m.\u001b[39mmean(aucs), \u001b[38;5;241m3\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'auc_score' is not defined"],"ename":"NameError","evalue":"name 'auc_score' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# apply model to validation dataset\npredictions = trainer.predict(tokenized_data[\"validation\"])\n\n# Extract the logits and labels from the predictions object\nlogits = predictions.predictions\nlabels = predictions.label_ids\n\n# Use your compute_metrics function\nmetrics = compute_metrics((logits, labels))\nprint(metrics)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T01:35:10.381186Z","iopub.execute_input":"2024-11-04T01:35:10.381595Z","iopub.status.idle":"2024-11-04T01:35:11.662455Z","shell.execute_reply.started":"2024-11-04T01:35:10.381559Z","shell.execute_reply":"2024-11-04T01:35:11.661533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Step 1: Make predictions\npredictions = trainer.predict(tokenized_data[\"test\"])\n\n# Convert predictions to class labels\npredicted_classes = np.argmax(predictions.predictions, axis=1)\ntrue_labels = predictions.label_ids\n\n# Step 2: Generate the confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_classes)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(4), yticklabels=range(4))\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n# Step 3: Print classification report\nclass_report = classification_report(true_labels, predicted_classes, target_names=[f\"Class {i}\" for i in range(4)])\nprint(\"Classification Report:\\n\", class_report)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T01:39:19.108424Z","iopub.execute_input":"2024-11-04T01:39:19.108854Z","iopub.status.idle":"2024-11-04T01:39:20.820964Z","shell.execute_reply.started":"2024-11-04T01:39:19.108819Z","shell.execute_reply":"2024-11-04T01:39:20.820031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_data[\"validation\"])\n\npredicted_classes = np.argmax(predictions.predictions, axis=1)\ntrue_labels = predictions.label_ids\n\nconf_matrix = confusion_matrix(true_labels, predicted_classes)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(4), yticklabels=range(4))\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nclass_report = classification_report(true_labels, predicted_classes, target_names=[f\"Class {i}\" for i in range(4)])\nprint(\"Classification Report:\\n\", class_report)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T01:42:56.028570Z","iopub.execute_input":"2024-11-04T01:42:56.029448Z","iopub.status.idle":"2024-11-04T01:42:57.555861Z","shell.execute_reply.started":"2024-11-04T01:42:56.029409Z","shell.execute_reply":"2024-11-04T01:42:57.554952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nimport numpy as np\n\n# Assuming predictions and true_labels are defined as in previous code\nfor i in range(4):\n    binary_labels = (true_labels == i).astype(int)\n    probabilities = predictions.predictions[:, i]\n\n    precision, recall, _ = precision_recall_curve(binary_labels, probabilities)\n    plt.plot(recall, precision, label=f\"Class {i}\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve for Each Class\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T01:44:42.571317Z","iopub.execute_input":"2024-11-04T01:44:42.572185Z","iopub.status.idle":"2024-11-04T01:44:42.861517Z","shell.execute_reply.started":"2024-11-04T01:44:42.572144Z","shell.execute_reply":"2024-11-04T01:44:42.860694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfor i in range(4):\n    binary_labels = (true_labels == i).astype(int)\n    probabilities = predictions.predictions[:, i]\n\n    fpr, tpr, _ = roc_curve(binary_labels, probabilities)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {roc_auc:.2f})\")\n\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Each Class\")\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T01:45:13.903012Z","iopub.execute_input":"2024-11-04T01:45:13.903771Z","iopub.status.idle":"2024-11-04T01:45:14.137354Z","shell.execute_reply.started":"2024-11-04T01:45:13.903733Z","shell.execute_reply":"2024-11-04T01:45:14.136530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T01:46:39.175985Z","iopub.execute_input":"2024-11-04T01:46:39.176382Z","iopub.status.idle":"2024-11-04T01:46:39.202449Z","shell.execute_reply.started":"2024-11-04T01:46:39.176345Z","shell.execute_reply":"2024-11-04T01:46:39.201649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# push model to hub\ntrainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T01:46:57.301895Z","iopub.execute_input":"2024-11-04T01:46:57.302841Z","iopub.status.idle":"2024-11-04T01:47:13.907357Z","shell.execute_reply.started":"2024-11-04T01:46:57.302802Z","shell.execute_reply":"2024-11-04T01:47:13.906427Z"},"trusted":true},"execution_count":null,"outputs":[]}]}